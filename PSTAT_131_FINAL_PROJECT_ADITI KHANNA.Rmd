---
title: 'PSTAT 131 Final Project: Sports Car Data'
author: "Aditi Khanna"
date: "2024-12-04"
output:  
  html_document:
    toc: true
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

In this project, we aim to predict the engine size (in liters) of sports cars based on various performance and design characteristics such as horsepower, torque, 0-60 mph time, and the car's price. Engine size is a critical factor in the performance of a car, influencing its power, fuel efficiency, and overall capabilities. By modeling engine size, we can gain insights into the relationships between a car’s specifications and its engine displacement.

This problem falls under the domain of regression analysis because the response variable, Engine Size, is continuous. The dataset contains 1,000 observations of sports cars from various luxury manufacturers such as Porsche, Ferrari, Lamborghini, and McLaren. The predictor variables include key numerical and categorical data such as horsepower, price, and torque, which are essential for understanding how engine size varies across different car models.

To ensure robust model performance, we stratified the data by Engine Size during the train-test split. Stratification was important to preserve the distribution of engine sizes in both training and testing datasets, ensuring fair evaluation of our model.

## Why?

Understanding the factors that influence engine size is essential for both car manufacturers and consumers. For manufacturers, engine size plays a key role in determining performance characteristics such as speed, torque, and efficiency, as well as compliance with market demands and environmental regulations. For consumers, engine size often reflects a balance between performance and affordability, influencing purchasing decisions. By approaching this problem, we aim to uncover how features like horsepower, torque, and acceleration correlate with engine size. This analysis not only helps predict engine size for new or existing car models but also provides insights into optimizing car design, improving performance, and better meeting consumer expectations.

## Pre-Processing

Now let's start exploring our data! We will start by loading in the necessary packages for our exploration, as seen below.

```{r}
## Loading necessary packages
library(conflicted)
library(ggplot2)
library(GGally)
library(corrplot)
library(knitr)
library(caret)
library(tidyverse)
library(recipes)
library(dplyr)
library(tidyr)
library(rsample)
library(parsnip)
library(workflows)
library(rsample)
library(recipes)
library(tune)
library(yardstick)
library(randomForest)
library(e1071)  # for SVM
library(xgboost)  # for Gradient Boosting
library(yardstick)  # for performance metrics
library(janitor)
library(car)
library(dials)
library(naniar)
library(stringi)
library(kernlab)
```

Next we read in our dataset. Below we can see the first 6 rows. 

```{r}
sport_car_data <- read.csv("Sport Car price.csv")
head(sport_car_data)
```

Some of the variables that we will be working with are car makes, car models, the year of production, engine size, horsepower, torque size, the time it takes to accelerate to 60 mph, and the price of the car. 

We can see the variables that we are working with as well as some of the values in each column. 

From the preview of the data set, we notice that the Price in USD column has commas, which could cause problems when we start working with the data. For simplicity, we will remove this to only have the price in USD without the commas. We will also need to convert the cleaned column to a numeric variable.

```{r}
# Remove commas and any non-numeric characters
sport_car_data$Price..in.USD. <- gsub(",", "", sport_car_data$Price..in.USD.)

# Convert the cleaned column to numeric
sport_car_data$Price..in.USD. <- as.numeric(sport_car_data$Price..in.USD.)

# Check if the conversion was successful
summary(sport_car_data$Price..in.USD.)

```
The conversion was successful since the numbers now appear without commas. From the summary statistics of the Price in USD column, we see that the minimum price is $25,000 and the maximum is $5,200,000. This large gap in prices tells us that some of the sports cars in the data set are likely luxury brands while others are more affordable.

Now we can look at the dimensions of our data to see how many rows and columns we will be working with. 

```{r}
dim(sport_car_data)
```
There are 1007 rows and 8 columns in our dataset.

## Exploratory Data Analysis

### Check Structure and Missing Data

Now we will start exploring the structure and values our dataset contains. We will start by checking the structure of the dataset and obtaining summary statistics for the whole data set.

```{r}
# Check the structure of the dataset
str(sport_car_data)

# Get a summary of the dataset
summary(sport_car_data)

```
The dataset contains information on 1,007 high-performance vehicles from various manufacturers, with a focus on key characteristics such as engine specifications, performance metrics, and price. The dataset includes the Car Make and Model as character variables, identifying the manufacturer and specific model of each car. The Year variable is numerical, ranging from 1965 to 2023, reflecting the production year of the vehicles. Engine specifications, such as Engine Size (in liters), Horsepower, and Torque (in lb-ft), are recorded as character variables, though they could be converted to numeric for further analysis. Performance metrics include the 0-60 MPH Time (in seconds), also recorded as a character variable. Finally, the Price (in USD) is a numerical variable that captures the cost of each car, ranging from $25,000 to a staggering $5,200,000, highlighting the inclusion of both standard and luxury models. 

The dataset contains several variables with character class types (Engine.Size..L., Horsepower, Torque..lb.ft., X0.60.MPH.Time..seconds., and Price..in.USD.), but many of these should be numeric variables for analysis. To proceed with analysis and modeling, we will need to convert these character columns into numeric format.

```{r}
# Convert relevant columns to numeric
sport_car_data$Horsepower <- as.numeric(sport_car_data$Horsepower)
sport_car_data$Torque..lb.ft. <- as.numeric(sport_car_data$Torque..lb.ft.)
sport_car_data$X0.60.MPH.Time..seconds. <- as.numeric(sport_car_data$X0.60.MPH.Time..seconds.)
sport_car_data$Engine.Size..L. <- as.numeric(sport_car_data$Engine.Size..L.)

# Verify the structure after conversion
str(sport_car_data)

# Identify any rows where conversion introduced NA values
colSums(is.na(sport_car_data))
```
Now the necessary variables have been converted to numeric variables. However, we see that NAs are introduced by coercion, so now we need to deal with the NA values since they will make it difficult when we start using the data to build models. Since our dataset is not too large and has a little over 1000 observations, we will set the missing values to the mean of the values in the column rather than removing the observation.

```{r}
# Gets rid of missing values, sets missing values to the mean of all values in the column
sport_car_data$Torque..lb.ft.[is.na(sport_car_data$Horsepower)] <- mean(sport_car_data$Torque..lb.ft., na.rm = TRUE)
sport_car_data$X0.60.MPH.Time..seconds.[is.na(sport_car_data$X0.60.MPH.Time..seconds.)] <- mean(sport_car_data$X0.60.MPH.Time..seconds., na.rm = TRUE)
sport_car_data$Torque..lb.ft.[is.na(sport_car_data$Torque..lb.ft.)] <- mean(sport_car_data$Torque..lb.ft., na.rm = TRUE)
sport_car_data$Horsepower[is.na(sport_car_data$Horsepower)] <- mean(sport_car_data$Horsepower, na.rm = TRUE)
sport_car_data$Engine.Size..L.[is.na(sport_car_data$Engine.Size..L.)] <- mean(sport_car_data$Engine.Size..L., na.rm = TRUE)
```

Now we should verify that the NA values have been properly removed before proceeding. We can check by adding up the number of NA values in each column to ensure the removal worked.

```{r}
colSums(is.na(sport_car_data))
```
All the columns have 0 NA values, so the removal worked successfully. Now we will check for multicollinearity before proceeding to visual exploratory data analysis.

### Multicollinearity

```{r}

# Check for multicollinearity using VIF
# Fit a linear model with Engine Size as the response variable
lm_model <- lm(Engine.Size..L. ~ Horsepower + Torque..lb.ft. + X0.60.MPH.Time..seconds. + Year + Price..in.USD., 
               data = sport_car_data)

# Calculate VIF for each predictor
vif_values <- vif(lm_model)

# Print the VIF values
print("Variance Inflation Factor (VIF) values:")
print(vif_values)

```
Multicollinearity refers to the situation where predictor variables are highly correlated with each other, potentially causing issues for certain statistical models. To evaluate multicollinearity in our dataset, we calculated the Variance Inflation Factor (VIF) for all numeric predictor variables. VIF values greater than 5 or 10 are commonly used thresholds for identifying multicollinearity.

These values suggest that Horsepower and Torque exhibit high multicollinearity, as their VIF values exceed the typical thresholds. Multicollinearity can lead to instability in coefficient estimation for some models, making it an important factor to consider during analysis.

We will deal with the implications of these findings in the modeling section, as some of the techniques used are inherently robust to multicollinearity. Now we will continue with our visual exploratory data analysis.

### Visual EDA

Visual exploratory data analysis will allow us to observe some of the relationships and patterns between variables in the dataset. We begin by looking at the distribution of important variables, such as Price, Engine Size, and Horsepower.

```{r}
# Histogram of Engine Size
ggplot(sport_car_data, aes(x = Engine.Size..L.)) +
  geom_histogram(binwidth = 0.1, fill = "skyblue", color = "black", alpha = 0.7) +
  geom_vline(aes(xintercept = median(Engine.Size..L., na.rm = TRUE)), 
             color = "red", linetype = "dashed", linewidth = 1) +
  theme_minimal() +
  labs(title = "Distribution of Engine Size",
       x = "Engine Size (Liters)",
       y = "Count")

```

The histogram above displays the distribution of engine sizes in the dataset, measured in liters. The red dashed line indicates the median engine size, which centers around 4 liters. The distribution appears somewhat bimodal, with noticeable peaks around 4 liters and another smaller peak near 6 liters. The dataset includes a wide range of engine sizes, spanning from smaller engines below 2 liters to larger engines approaching 8 liters. This indicates that the dataset captures a diverse array of sports cars, from more compact models to high-performance vehicles with larger engines. The concentration around the median suggests that mid-sized engines dominate the dataset.

Now let's create a scatter plot of engine size vs. horsepower.

```{r}
# Scatter Plot of Engine Size vs. Horsepower
ggplot(sport_car_data, aes(x = Engine.Size..L., y = Horsepower)) +
  geom_point(color = "steelblue", alpha = 0.7) +
  geom_smooth(method = "lm", se = TRUE, color = "red") +
  theme_minimal() +
  labs(title = "Relationship Between Engine Size and Horsepower",
       x = "Engine Size (Liters)",
       y = "Horsepower")

```


The scatterplot above illustrates the relationship between engine size (in liters) and horsepower. Each point represents a sports car in the dataset. A positive correlation is observed, with larger engine sizes generally associated with higher horsepower. The red line represents the line of best fit, indicating a clear upward trend. This suggests that as engine size increases, the power output of the car also tends to rise. However, there is some variability in the data, as indicated by points that deviate significantly from the trendline, particularly for engines above 6 liters, where horsepower values vary widely. This reflects differences in how efficiently power is generated by larger engines across different car models.

Now let's create a scatter plot of price vs. horsepower.

```{r}
# Scatter Plot of Price vs. Horsepower
ggplot(sport_car_data, aes(x = Horsepower, y = Price..in.USD.)) +
  geom_point(color = "darkgreen", alpha = 0.7) +
  geom_smooth(method = "lm", se = TRUE, color = "red") +
  theme_minimal() +
  labs(title = "Relationship Between Horsepower and Price",
       x = "Horsepower (HP)",
       y = "Price (USD)")

```

This graph illustrates the relationship between a vehicle's horsepower and its price, showing a general trend where higher horsepower is associated with higher prices. The red line represents a linear trend line, suggesting a positive correlation between these two variables. However, the data points, shown as green dots, highlight that this relationship is not perfect. There is significant variability in prices for vehicles with similar horsepower, especially at the lower and middle ranges. At higher horsepower levels (above 1000 HP), prices become even more scattered, indicating that other factors, such as brand, design, or exclusivity, may influence the price. The graph suggests that while horsepower is a key driver of price, it is not the sole determinant.

Now let's create a box plot of engine size by year.

```{r}
# Box Plot of Engine Size by Year
ggplot(sport_car_data, aes(x = factor(Year), y = Engine.Size..L.)) +
  geom_boxplot(fill = "lightgreen", color = "darkgreen", outlier.color = "red", outlier.size = 2) +
  theme_minimal() +
  labs(title = "Engine Size Distribution by Year",
       x = "Year",
       y = "Engine Size (Liters)") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

The graph reveals a shift in engine size distributions over time. Earlier years (e.g., 1965 and 2014) have limited data, but from 2019 onward, engine sizes are more consistently recorded. In recent years (2020–2023), engine sizes show greater variability, with larger ranges and more outliers, particularly in 2021 and 2022. This suggests increased diversity in engine types, possibly reflecting a mix of smaller, fuel-efficient engines and larger, high-performance ones. The trend highlights evolving automotive design trends over the decades.

Now let's create a box plot of 0-60 MPH time by Car Make.

```{r}
# Box Plot of 0-60 MPH Time by Car Make
ggplot(sport_car_data, aes(x = reorder(Car.Make, X0.60.MPH.Time..seconds.), y = `X0.60.MPH.Time..seconds.`, fill = Car.Make)) +
  geom_boxplot(outlier.color = "red", outlier.shape = 16, alpha = 0.7) +
  theme_minimal() +
  coord_flip() +
  labs(title = "0-60 MPH Time Across Car Makes",
       x = "Car Make",
       y = "0-60 MPH Time (seconds)") +
  theme(legend.position = "none")

```

The graph shows significant variation in performance between car brands. High-performance brands like Rimac, Koenigsegg, and Ferrari are clustered at the lower end, with extremely fast acceleration times (around 2 seconds). In contrast, luxury brands like Rolls-Royce and Jaguar generally have higher 0-60 times, reflecting a focus on comfort rather than speed. Outliers for many brands suggest specialized models that deviate from their typical performance. This chart highlights the diversity of vehicle capabilities, influenced by brand priorities such as luxury, performance, or economy.

Now we will create a bubble plot Price vs Horsepower, colored by Torque and sized by Engine Size to further explore relationships.

```{r}
# Bubble Plot: Price vs Horsepower, colored by Torque and sized by Engine Size
ggplot(sport_car_data, aes(x = Horsepower, y = Price..in.USD., color = Torque..lb.ft., size = Engine.Size..L.)) +
  geom_point(alpha = 0.7) +
  scale_color_gradient(low = "lightblue", high = "darkred") +
  theme_minimal() +
  labs(title = "Price vs. Horsepower, Colored by Torque and Sized by Engine Size",
       x = "Horsepower (HP)",
       y = "Price (USD)",
       color = "Torque (lb-ft)",
       size = "Engine Size (Liters)")

```

The plot shows a positive orrelation between Horsepower and Price: As horsepower increases along the x-axis, the general trend shows vehicle prices rising as well. This indicates that more powerful engines tend to command higher prices in the sports car market.

The coloring and sizing of the data points provide additional context. Higher torque outputs, represented by the warmer red/brown hues, are clustered towards the upper right portion of the plot - the most powerful and expensive vehicles. Similarly, the largest engine sizes (8L) are found in this high-performance, high-cost quadrant. Smaller engine displacements (2L, 4L, 6L) are more prevalent in the lower horsepower, lower price range.

While the overall trend is positive, there are clear outliers that diverge from the general pattern. Some vehicles with relatively modest horsepower (around 1,000 HP) have very high prices, likely due to factors beyond just raw power, such as luxury features, brand cachet, or specialized performance capabilities. Conversely, a few high-horsepower models appear to be priced lower than expected based on the broader trend.

Lastly we will create a heatmap to observe correlations between variables. 

```{r}
# Select only numeric columns for the correlation matrix
numeric_data <- sport_car_data %>%
  select(Engine.Size..L., Horsepower, Price..in.USD.)

# Select all numeric columns for the correlation matrix
numeric_data_all <- sport_car_data %>%
  select(where(is.numeric))

# Calculate the correlation matrix for all numeric variables
cor_matrix_all <- cor(numeric_data_all, use = "complete.obs")  # Use complete cases to handle missing data


# Create the correlation plot
corrplot(cor_matrix_all, 
         method = "circle",  # You can change the method to "square", "ellipse", etc.
         type = "upper",  # Show only the upper triangle
         order = "hclust",  # Arrange by hierarchical clustering
         tl.col = "black",  # Color of text labels
         tl.srt = 45,  # Rotate text labels
         addCoef.col = "black",  # Add correlation coefficients to the plot
         diag = FALSE)  # Don't show diagonal (correlation with itself)

```

This heatmap illustrates the correlation between various car features, including engine size, price, horsepower, torque, year, and 0-60 mph acceleration time. The color gradient and numerical values highlight the strength and direction of these relationships. A strong positive correlation exists between horsepower and torque (0.94), as well as between horsepower and price (0.79), indicating that cars with more horsepower tend to generate higher torque and are priced higher. Additionally, a moderate negative correlation between 0-60 mph acceleration time and horsepower (-0.72) suggests that cars with greater horsepower achieve faster acceleration. Engine size also shows moderate positive correlations with price (0.36), horsepower (0.43), and torque (0.43), emphasizing its influence on performance and cost. These correlations offer valuable insights into how various attributes interact to impact vehicle performance and pricing.

Now that we have explored some of the relationships and patterns in our data, we have gained a better understanding of the overall data set. It is time to start splitting our data. We will randomly split our data into training and testing sets, set up and create our recipe, and establish cross-validation within our models.

## Data Split

We will proceed with splitting the data. We will do a 70/30 split where 70% is part of the training set and 30% is part of the testing set.

One will be used for the training of our models, and one will be the testing set, which is used only once when we actually test our models. First we will need to set a seed so that the random split can be reproduced every time the models are trained. Then, we will perform the split on our data and stratify on our response variable, Engine.Size..L.

```{r}
# Split the data into training and testing sets (70% train, 30% test)

set.seed(200)  # Set a seed for reproducibility

split <- initial_split(sport_car_data, prop = 0.7, strata = Engine.Size..L.)

# Extract training and testing sets

train_data <- training(split)
test_data <- testing(split)
```

### Verify the Split 

Let's verify that the data split correctly before we move on. We can divide the number of rows of the train and test data by the total rows in the sports car dataset.

```{r}
nrow(train_data)/nrow(sport_car_data)
```

This is about 70%, which is correct.
 
```{r}
nrow(test_data)/nrow(sport_car_data)
```

This is about 30% which is also correct. 

We can now proceed since the data split correctly.

## Building the Recipe

The next step is to use our predictor variables and the response variable, Engine Size, to build the recipe that will be used for all the models. Each model will use this recipe and work with it. We will be using the same 6 predictor variables: Car.Make, Car.Model, Year, Horsepower, Torque..lb.ft., and X0.60.MPH.Time..seconds. 

To handle categorical data, we convert categorical variables such as Car Make and Car Model into dummy variables using one-hot encoding, allowing machine learning algorithms to process them effectively. We also address potential issues with unseen levels in categorical variables by using a step to handle novel categories. Furthermore, we remove predictors with zero variance, as they do not add any value to the model. Finally, we normalize numeric variables to ensure they are on the same scale, which helps prevent certain predictors from dominating others due to their larger ranges. These preprocessing steps help create a robust data set that is optimized for accurate and efficient model training.

```{r}
# Modify the recipe to remove zero variance columns
car_recipe <- recipe(Engine.Size..L. ~ Car.Make + Car.Model + Year + Horsepower + Torque..lb.ft. + X0.60.MPH.Time..seconds., data = sport_car_data) %>%
  
  step_novel(all_nominal(), -all_outcomes()) %>%  # Handle novel levels
  
  step_dummy(all_nominal(), -all_outcomes()) %>%  # Convert categorical variables to dummies
  
  step_zv(all_predictors()) %>%  # Remove zero variance predictors
  
  step_normalize(all_numeric(), -all_outcomes())  # Normalize the numeric predictors

```

The recipe has been created successfully.

## K Fold Validation

Now we will create folds to conduct k-fold stratified cross validation. The model is trained on (k-1) folds and tested on the remaining fold, repeating the process k times so that each fold serves as a test set exactly once. 

By ensuring that the model is evaluated on different portions of the data, K-fold cross-validation reduces the bias associated with a single train-test split and provides a more comprehensive understanding of how well the model will perform on unseen data. This technique is particularly useful for small datasets, as it makes efficient use of all available data for both training and validation.

We stratify on the outcome, Engine.Size..L., to make sure that the data in each fold is not imbalanced.

```{r}
# Creating folds
car_folds <- vfold_cv(train_data, v = 10, strata = Engine.Size..L.)
```

## Model Building

Now that we have defined a recipe and created folds, it is time to start building our models. For each model, we will define the model, workflow, and evaluate its performance using the training data. The workflow combines the preprocessing steps defined in the recipe with the chosen model. Each model will be trained using the training data and evaluated through K-fold cross-validation, which will provide performance metrics such as RMSE to measure accuracy. Once trained, the models will be tested on the test dataset to assess their ability to generalize to unseen data. By comparing the performance of various models, we can identify the most effective approach for predicting Engine Size based on the provided features. 

Each of the models had a very similar process. Specifically, for each of the models we:

1. Specified the type of regression or machine learning algorithm to be used. For instance, Ridge Regression included L2 regularization, Elastic Net used a combination of both, and Support Vector Machines (SVM) utilized kernel-based methods.

2. Created a Workflow by
using the workflow() function to combine the recipe (data preprocessing steps) with the specific model. This workflow structure ensures consistent preprocessing across all models and simplifies the model-fitting process.

3. Each workflow was then trained on the training dataset using cross-validation. This step allowed us to assess how the model performed across different splits of the data and ensured that the training process was robust and unbiased.
 
4. After fitting the models, we used them to generate predictions on the test dataset, providing the basis for evaluating their performance.

Let's begin with Ridge Regression. We perform the steps outlined above. 

### Ridge Regression

Penalty: The regularization strength, controlled by the penalty parameter. A higher penalty leads to more regularization (shrinkage of coefficients), while a lower penalty reduces regularization, allowing the model to fit more flexibly to the data.

Mixture: Set to 0 to specify L2 regularization, ensuring that Ridge Regression is applied (as opposed to Lasso or Elastic Net).

Engine: glmnet, which is a popular R package for fitting regularized linear models.

Mode: Regression, as the task is to predict a continuous variable (Engine Size).
The penalty parameter is tuned by performing cross-validation over a grid of possible values to find the one that minimizes RMSE (Root Mean Squared Error), a common metric for evaluating the performance of regression models.

Tuning Process:
To find the optimal value for the penalty parameter, I created a grid of possible values ranging from 0.001 to 1. The grid was regular, with 20 levels, allowing for a good search of the penalty range.

```{r}
# Modify Ridge model to tune penalty
ridge_model <- linear_reg(penalty = tune(), mixture = 0) %>%  # Mixture = 0 ensures Ridge (L2 regularization)
  set_engine("glmnet") %>%
  set_mode("regression")

# Create grid of penalty values
penalty_grid <- grid_regular(penalty(range = c(0.001, 1)), levels = 20)

# Tune the Ridge model
ridge_tuned <- tune_grid(
  workflow() %>%
    add_recipe(car_recipe) %>%
    add_model(ridge_model),
  resamples = car_folds,  # Cross-validation folds
  grid = penalty_grid,
  metrics = metric_set(rmse)
)

# View tuning results
ridge_tuned

# Select the best penalty value
best_ridge <- select_best(ridge_tuned, metric = "rmse")
print(best_ridge)

# Refit Ridge model with the best penalty
final_ridge_model <- finalize_workflow(
  workflow() %>%
    add_recipe(car_recipe) %>%
    add_model(ridge_model),
  best_ridge
)

# Fit the final Ridge model on the training data
final_ridge_fit <- fit(final_ridge_model, train_data)

# View the fitted model
print(final_ridge_fit)
# Make predictions on the test set
ridge_predictions <- predict(final_ridge_fit, test_data)

# Combine predictions with actual values
test_results <- test_data %>%
  select(Engine.Size..L.) %>%
  bind_cols(ridge_predictions)

# Evaluate performance
test_performance <- test_results %>%
  metrics(truth = Engine.Size..L., estimate = .pred)

print(test_performance)


```

Next we implemented Elastic Net Regression. 

### Elastic Net 

Penalty: Controls the strength of the regularization. A larger value imposes more regularization (shrinkage of coefficients).

Mixture: Balances between Lasso (mixture = 1) and Ridge (mixture = 0). Values between 0 and 1 allow for a mixture of both regularizations.

Penalty: Varies from 0.001 to 1.
Mixture: Varies from 0 (Ridge) to 1 (Lasso).
Levels:  20 levels for penalty values and 5 levels for the mixture.

```{r}
# Modify Elastic Net model to tune both penalty and mixture
elastic_net_model <- linear_reg(penalty = tune(), mixture = tune()) %>%  # Both penalty and mixture are tuned
  set_engine("glmnet") %>%
  set_mode("regression")

# Create grid for penalty and mixture values
penalty_grid <- grid_regular(
  penalty(range = c(0.001, 1)),  # Explore penalty from 0.001 to 1
  mixture(range = c(0, 1)),      # Mixture can vary from 0 (Ridge) to 1 (Lasso)
  levels = c(20, 5)              # 20 levels for penalty, 5 levels for mixture
)

# Tune the Elastic Net model using cross-validation
elastic_net_tuned <- tune_grid(
  workflow() %>%
    add_recipe(car_recipe) %>%
    add_model(elastic_net_model),
  resamples = car_folds,          # Cross-validation folds
  grid = penalty_grid,            # Use the grid for tuning
  metrics = metric_set(rmse)      # Use RMSE as the evaluation metric
)

# View tuning results
elastic_net_tuned

# Select the best combination of penalty and mixture based on RMSE
best_elastic_net <- select_best(elastic_net_tuned, metric = "rmse")
print(best_elastic_net)

# Refit the Elastic Net model with the best hyperparameters
final_elastic_net_model <- finalize_workflow(
  workflow() %>%
    add_recipe(car_recipe) %>%
    add_model(elastic_net_model),
  best_elastic_net
)

# Fit the final Elastic Net model on the training data
final_elastic_net_fit <- fit(final_elastic_net_model, train_data)

# View the fitted model
print(final_elastic_net_fit)

# Make predictions on the test set
elastic_net_predictions <- predict(final_elastic_net_fit, test_data)

# Combine predictions with actual values
test_results <- test_data %>%
  select(Engine.Size..L.) %>%
  bind_cols(elastic_net_predictions)

# Evaluate performance on the test set
test_performance <- test_results %>%
  metrics(truth = Engine.Size..L., estimate = .pred)

print(test_performance)


```

Next we implemented SVM (Support Vector Machine) Model. 

### SVM 

Cost (C): This parameter determines the penalty for misclassified points. A higher cost implies a smaller margin and more penalties for misclassified data, leading to a more complex model.
rbf_sigma: This parameter controls the width of the radial basis function kernel. A smaller value leads to a model that can fit more closely to the training data (overfitting risk), while a larger value can lead to a smoother decision boundary (underfitting risk).

The grid will search for values of cost between 0.1 and 10 and rbf_sigma between 0.01 and 1, testing 5 evenly spaced values for each.

```{r}
# SVM Regression model with tuning
svm_model <- svm_rbf(
  cost = tune(),       # Tune the cost parameter
  rbf_sigma = tune()   # Tune the RBF kernel's sigma parameter
) %>%
  set_engine("kernlab") %>%
  set_mode("regression")

# Create a tuning grid
svm_grid <- grid_regular(
  cost(range = c(0.1, 10)),        # Search range for cost parameter
  rbf_sigma(range = c(0.01, 1)),  # Search range for sigma parameter
  levels = 5                      # Number of grid levels
)

# Create workflow for SVM Regression
svm_workflow <- workflow() %>%
  add_recipe(car_recipe) %>%
  add_model(svm_model)

# Tune the SVM model using cross-validation
svm_tuned <- tune_grid(
  svm_workflow,
  resamples = car_folds,     # Use the pre-defined cross-validation folds
  grid = svm_grid,           # Hyperparameter grid
  metrics = metric_set(rmse, rsq)  # Evaluation metrics
)

# Collect metrics for tuning results
collect_metrics(svm_tuned)

# Select the best hyperparameters based on RMSE
best_params <- select_best(svm_tuned, metric = "rmse")
print(best_params)

# Finalize the SVM workflow with the best parameters
final_svm_workflow <- finalize_workflow(svm_workflow, best_params)

# Fit the final SVM model to the training data
final_svm_model <- fit(final_svm_workflow, data = train_data)

# Evaluate on the testing set
svm_predictions <- predict(final_svm_model, new_data = test_data) %>%
  bind_cols(test_data)

# Calculate performance metrics
svm_performance <- svm_predictions %>%
  metrics(truth = Engine.Size..L., estimate = .pred)

print(svm_performance)


```


Lastly, we implemented Random Forest model. 

### Random Forest

In the Random Forest model, the key parameters that were tuned are as follows:

mtry (Number of Predictors to Consider at Each Split):

This parameter determines how many predictors are randomly selected at each split in the decision trees. Tuning mtry helps improve model performance by preventing overfitting. We set this parameter to be tuned over a range of values from 1 to 6.

trees (Number of Trees in the Forest):
This specifies the total number of trees in the forest. A higher number of trees improves model performance and stability, but also increases computation time. We tuned the number of trees between 200 and 600.

min_n (Minimum Number of Data Points in a Node):
This parameter sets the minimum number of data points required to make a split in a decision tree. Larger values help prevent overfitting by controlling the depth of the tree. We tuned this value in the range of 10 to 20.
These parameters were tuned using a grid search approach, where we specified a range of values and evaluated the performance for each combination of hyperparameters.

```{r}
# Random Forest Model Specification (for regression)
rf_reg_spec <- rand_forest(mtry = tune(), 
                           trees = tune(),
                           min_n = tune()) %>%
  set_engine("ranger") %>%
  set_mode("regression")

# Workflow for Random Forest
rf_reg_wf <- workflow() %>% 
  add_model(rf_reg_spec) %>% 
  add_recipe(car_recipe)

# Tuning Grid for Random Forest
rf_grid <- grid_regular(mtry(range = c(1, 6)), trees(range = c(200, 600)),
                        min_n(range = c(10, 20)), levels = 5)
rf_grid

# Tune the Random Forest Model
tune_rf <- tune_grid(rf_reg_wf, resamples = car_folds, grid = rf_grid)

# Plot the tuning results
autoplot(tune_rf) + theme_minimal()

# Extract the best model from the tuning
best_rf_reg <- select_best(tune_rf)

# Finalizing and Fitting the Best Random Forest Model
final_rf_model <- finalize_workflow(rf_reg_wf, best_rf_reg)
final_rf_model <- fit(final_rf_model, train_data)

# Evaluate on Test Data
rf_preds <- predict(final_rf_model, new_data = test_data)
rf_rmse <- sqrt(mean((test_data$Engine.Size - rf_preds$.pred)^2))

cat("Random Forest RMSE (Testing):", rf_rmse, "\n")

```

We have now built our models. Now let's discuss the model results.

## Model Results

### 1. Now we predict each model on our test set to get the model's performance.

```{r}
# Ridge model results
ridge_results <- fit(final_ridge_model, train_data)

# Elastic Net model results
elastic_net_results <- fit(final_elastic_net_model, train_data)

# SVM model results
svm_results <- fit(final_svm_model, train_data)

# Random Forest model results
rf_results <- final_rf_model

##----

# Ridge Predictions
ridge_preds <- predict(ridge_results, new_data = test_data)$.pred

# Elastic Net Predictions
elastic_net_preds <- predict(elastic_net_results, new_data = test_data)$.pred

# SVM Predictions
svm_preds <- predict(svm_results, new_data = test_data)$.pred

# Random Forest Predictions
rf_preds <- predict(rf_results, new_data = test_data)$.pred


```

### 2. Then, we create a data frame for each model. This will store the model results and be useful in the future.

```{r}
# Create a data frame for Ridge model results
ridge_results_df <- data.frame(truth = test_data$Engine.Size..L., prediction = ridge_preds)

# Create a data frame for Elastic Net model results
elastic_net_results_df <- data.frame(truth = test_data$Engine.Size..L., prediction = elastic_net_preds)

# Create a data frame for SVM model results
svm_results_df <-
  data.frame(truth =
  test_data$Engine.Size..L., prediction = svm_preds)

## Create a data frame for Random Forest results
rf_results_df <- data.frame(truth = test_data$Engine.Size..L., prediction = rf_preds)
```

### 3. Now, we calculate the RMSE (root mean squared error) for each model. We use the data frames created in the above step. 

```{r}
# Calculate RMSE for Ridge model
ridge_rmse <- rmse(ridge_results_df, truth = truth, estimate = prediction)
print(paste("Ridge RMSE: ", ridge_rmse$.estimate))

# Calculate RMSE for Elastic Net model
elastic_net_rmse <- rmse(elastic_net_results_df, truth = truth, estimate = prediction)
print(paste("Elastic Net RMSE: ", elastic_net_rmse$.estimate))


# Calculate RMSE for SVM model
svm_rmse <-
  rmse(svm_results_df, truth = truth, estimate = prediction)
print(paste("SVM RMSE: ", svm_rmse$.estimate))

# Calculate RMSE for Random Forest
rf_rmse <- sqrt(mean((rf_results_df$truth - rf_results_df$prediction)^2))
print(paste("Random Forest RMSE: ", rf_rmse))
```

### 4. Now it is time to combine the results. We create one data frame that contains the RMSE values for all of the models. This is useful for comparing the model performances. 

```{r}
# Create a data frame with RMSE values for each model
rmse_results <- data.frame(
  Model = c("Ridge", "Elastic Net", "Support Vector Machine", "Random Forest"),
  RMSE = c(ridge_rmse$.estimate, elastic_net_rmse$.estimate, svm_rmse$.estimate, rf_rmse)
)
```

### 5. Lastly, we plot the RMSE values for each model to visualize the performances.

```{r}
# Plot RMSE values for each model with colors
ggplot(rmse_results, aes(x = Model, y = RMSE, fill = Model)) +
  geom_bar(stat = "identity", show.legend = FALSE) +  # Fill bars with color based on the model
  theme_minimal() +
  labs(title = "RMSE Comparison of Models", y = "RMSE", x = "Model") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +  # Rotate x-axis labels
  scale_fill_manual(values = c("Ridge" = "blue",       # Color for Ridge
                              "Elastic Net" = "black", # Color for Elastic Net
                              "SVM" = "darkblue",          # Color for SVM
                              "Random Forest" = "red"))  # Color for Random Forest

```

The RMSE is a common metric used to evaluate the performance of regression models, where lower values indicate better fit and predictive accuracy. In this case, the chart clearly shows that the Ridge regression and Elastic Net models have the lowest RMSE, both around 0.58.

This suggests the Ridge and Elastic Net approaches are the top performing models out of the four evaluated. The similar RMSE values for these two methods indicates they are achieving comparable levels of predictive accuracy on the target variable.

In contrast, the SVM model has a slightly higher RMSE of around 0.61, while the Random Forest model has the highest RMSE at approximately 0.61 as well. These higher error rates imply the SVM and Random Forest do not fit the data as well as the Ridge and Elastic Net for this particular modeling task.

The additional RMSE values provided in the text confirm these findings:

- Ridge RMSE: 0.576734083663277
- Elastic Net RMSE: 0.576734083663277
- SVM RMSE: 0.606513323099678 
- Random Forest RMSE: 0.609082263466736

The nearly identical RMSE scores for the Ridge and Elastic Net models suggest they are likely achieving very similar predictive performance. This could be due to the datasets and problem at hand favoring the regularized linear modeling approaches of Ridge and Elastic Net over the more complex non-linear models of SVM and Random Forest.

Overall, the chart and RMSE values provide a clear visual and quantitative comparison of how these four machine learning techniques perform on the target variable. The Ridge regression and Elastic Net models emerge as the top performers based on this evaluation metric. These insights could help inform model selection and tuning decisions for this particular predictive modeling task.

Now we can use our best two models to predict the testing set.

### Using the Best Two Models to Predict the Testing Set

```{r}
# Predict using Ridge Regression (best model)
ridge_preds_test <- predict(ridge_results, new_data = test_data)$.pred
# Create a data frame for Ridge Regression model results
ridge_results_df_test <- data.frame(truth = test_data$Engine.Size..L., prediction = ridge_preds_test)
# Calculate RMSE for Ridge Regression
ridge_rmse_test <- rmse(ridge_results_df_test, truth = truth, estimate = prediction)
cat("Ridge RMSE (Testing):", ridge_rmse_test$.estimate, "\n")
# Calculate Testing Error for Ridge
ridge_testing_error <- ridge_results_df_test$prediction - ridge_results_df_test$truth
ridge_testing_error_mean <- mean(abs(ridge_testing_error))
cat("Ridge Mean Absolute Testing Error:", ridge_testing_error_mean, "\n")

# Predict using SVM (second best model)
svm_preds_test <- predict(svm_results, new_data = test_data) %>%
  pull(.pred)
# Create a data frame for SVM model results
svm_results_df_test <- data.frame(truth = test_data$Engine.Size..L., prediction = svm_preds_test)
# Calculate RMSE for SVM
svm_rmse_test <- rmse(svm_results_df_test, truth = truth, estimate = prediction)
cat("SVM RMSE (Testing):", svm_rmse_test$.estimate, "\n")
# Calculate Testing Error for SVM
svm_testing_error <- svm_results_df_test$prediction - svm_results_df_test$truth
svm_testing_error_mean <- mean(abs(svm_testing_error))
cat("SVM Mean Absolute Testing Error:", svm_testing_error_mean, "\n")

# Create a data frame with the RMSE values and testing errors for both models
rmse_results_test <- data.frame(
  Model = c("Ridge", "SVM"),
  RMSE = c(ridge_rmse_test$.estimate, svm_rmse_test$.estimate),
  Mean_Absolute_Testing_Error = c(ridge_testing_error_mean, svm_testing_error_mean)
)

# Print the RMSE and Mean Absolute Testing Error for both models
print(rmse_results_test)

# Plot RMSE values for both models
ggplot(rmse_results_test, aes(x = Model, y = RMSE, fill = Model)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  theme_minimal() +
  labs(title = "RMSE Comparison of Ridge and SVM on Testing Data", y = "RMSE", x = "Model") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +  # Rotate x-axis labels
  scale_fill_manual(values = c("Ridge" = "blue", "SVM" = "red"))

# Plot Mean Absolute Testing Error for both models
ggplot(rmse_results_test, aes(x = Model, y = Mean_Absolute_Testing_Error, fill = Model)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  theme_minimal() +
  labs(title = "Mean Absolute Testing Error Comparison of Ridge and SVM", 
       y = "Mean Absolute Testing Error", x = "Model") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
  scale_fill_manual(values = c("Ridge" = "blue", "SVM" = "red"))

```

Based on the model performance metrics, Ridge Regression appears to be the best-fitting model for this dataset. With an RMSE of 0.5767 and an MAE of 0.4386, it provides a well-balanced fit, minimizing both overall prediction error (RMSE) and the average absolute error in individual predictions (MAE). The model is effectively capturing the underlying patterns in the data without overfitting, making it a strong candidate for accurate and reliable predictions. While Support Vector Machine (SVM) shows a slightly lower MAE of 0.2983, its higher RMSE of 0.6065 suggests that it may experience larger prediction errors, potentially due to sensitivity to outliers. Overall, Ridge Regression offers better predictive performance on average, but SVM could still be considered if minimizing absolute errors is more important. Given these results, Ridge Regression is the more robust and reliable choice for this problem, balancing error minimization and model simplicity.


```{r}
# Select numeric variables only for correlation
numeric_data <- sport_car_data %>%
  select(where(is.numeric))  # Retain only numeric columns

# Calculate correlations with the response variable (Engine Size)
correlations <- numeric_data %>%
  select(-Engine.Size..L.) %>%  # Exclude the response variable temporarily
  summarise(across(everything(), ~ cor(.x, numeric_data$Engine.Size..L., use = "complete.obs"))) %>%
  pivot_longer(everything(), names_to = "Predictor", values_to = "Correlation") %>%
  mutate(abs_corr = abs(Correlation)) %>%  # Calculate absolute correlation
  arrange(desc(abs_corr))  # Sort by absolute correlation

# Bar plot of variable importance based on absolute correlation
ggplot(correlations, aes(x = reorder(Predictor, abs_corr), y = abs_corr)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  coord_flip() +  # Flip for better readability
  theme_minimal() +
  labs(
    title = "Variable Importance Based on Correlation with Engine Size",
    x = "Predictor Variables",
    y = "Absolute Correlation"
  ) +
  scale_fill_viridis_d()
```

Horsepower and Torque were the most important predictor variables based on correlation with engine size while year was the lowest. This makes sense logically and reflects the patterns within our data.

## Handling Multicollinearity

As our analysis involved using Ridge Regression, Lasso Regression, Elastic Net, Support Vector Machines (SVM), and Random Forest, these models are inherently robust to multicollinearity in different ways:

### Ridge Regression: 
This model includes L2 regularization, which penalizes large coefficients and reduces the impact of multicollinearity by shrinking coefficients of correlated predictors. This makes it useful when predictors are highly correlated.

### Elastic Net: 
The Elastic Net combines both L1 and L2 regularization, providing the advantages of both Ridge and Lasso. It effectively handles multicollinearity and also performs variable selection, making it suitable when predictors are correlated or when the number of predictors exceeds the number of observations.

### SVM (Support Vector Machines): 
SVM works by maximizing the margin between classes and is generally robust to multicollinearity. It doesn't rely on a direct estimation of the coefficients like Ridge or Lasso, so multicollinearity doesn't have the same detrimental effect on SVM models.

### Random Forest: 
Random Forest, an ensemble model based on decision trees, is highly robust to multicollinearity. Since it builds multiple decision trees by randomly selecting subsets of features and training them on different parts of the data, it is less sensitive to correlated features. In fact, Random Forest can handle multicollinearity effectively by selecting relevant features based on the importance of each variable in predicting the target, without being severely affected by the correlations among predictors.

Thus, while multicollinearity was identified in the dataset, no explicit steps (e.g., variable removal) were taken to address it, as the models used in our analysis—Ridge Regression, Lasso Regression, Elastic Net, SVM, and Random Forest—are designed to manage multicollinearity effectively through their inherent regularization or feature selection mechanisms.


## Conclusion

After fitting and evaluating several regression models, the results offer valuable insights into the performance of each model for predicting Engine Size. Overall, Ridge Regression emerged as the best-performing model, with an RMSE of 0.5767 and a Mean Absolute Error (MAE) of 0.4386, indicating both low prediction errors and reliable performance. This model appears to strike an optimal balance between bias and variance, effectively capturing the underlying relationships in the data without overfitting. Given its excellent predictive accuracy, Ridge Regression is the most reliable choice for this problem.

On the other hand, Support Vector Machine (SVM) performed decently with an RMSE of 0.6065 and a lower MAE of 0.2983. While SVM’s absolute prediction errors were smaller on average, its higher RMSE suggested that it may struggle with larger deviations in some instances, possibly due to sensitivity to outliers or non-linear relationships. Despite this, SVM showed competitive performance and could be considered as an alternative to Ridge Regression in cases where fine-tuning hyperparameters or dealing with complex feature interactions is required.

Elastic Net, which combines elements of both Ridge and Lasso regression, showed performance comparable to Ridge with an RMSE of 0.5767. However, it had a slightly higher MAE, suggesting that while it can handle collinearity in the data effectively, it may not always offer the same level of precision as Ridge Regression, especially in cases where feature selection is more critical.

Random Forest performed similarly to SVM, with an RMSE of 0.6091, indicating that while it is generally robust and flexible, it did not quite match the predictive power of Ridge Regression or SVM. The random nature of the model might have contributed to this, as decision trees in the forest can sometimes overfit to certain parts of the data, leading to higher overall prediction errors.

### Were You Surprised by Model Performance?

The solid performance of Ridge Regression, with its straightforward regularization approach, was somewhat expected, as Ridge tends to work well when there are many correlated features and no obvious sparsity in the data. The Elastic Net’s close performance to Ridge was also expected, given its hybrid nature.

The Random Forest and SVM results were somewhat mixed, with both models showing slightly worse performance than Ridge. Random Forest’s higher computational complexity may have contributed to its slightly poorer performance, while SVM’s sensitivity to outliers may explain the higher RMSE despite its lower MAE.

### Next Steps

Given the current model performance, the next steps could include further hyperparameter tuning for SVM and Random Forest models to see if their performance can be improved. Specifically, SVM might benefit from optimizing the choice of kernel or tuning the cost parameter further, while Random Forest could be improved by refining the number of trees and the min_n parameter.

Additionally, exploring feature engineering techniques might improve model performance. For instance, creating new features that capture non-linear relationships or interactions between variables could potentially benefit the Random Forest and SVM models. 

These findings reinforce the importance of model selection, cross-validation, and hyperparameter tuning in achieving optimal performance. Moving forward, fine-tuning the models and exploring additional feature engineering could lead to further improvements in predictive accuracy.

```{r, echo=FALSE}
## Create a Codebook

# Create the codebook data frame
codebook_df <- data.frame(
  "Variable" = c("Car Make", "Car Model", "Year", "Engine Size (L)", 
                 "Horsepower", "Torque (lb-ft)", "0-60 MPH Time (seconds)", "Price (in USD)"),
  "Description" = c("The brand or manufacturer of the sports car.",
                    "The specific model of the sports car.",
                    "The year the sports car was manufactured.",
                    "The size of the engine in liters, representing the total volume of the engine’s cylinders.",
                    "The amount of power produced by the car’s engine.",
                    "The rotational force generated by the engine, in pound-feet (lb-ft).",
                    "The time it takes for the car to accelerate from 0 to 60 miles per hour.",
                    "The price of the car in U.S. dollars."),
  "Data Type" = c("Categorical (String)", "Categorical (String)", "Integer (Numeric)", 
                  "Numeric (Float)", "Numeric (Integer)", "Numeric (Integer)", 
                  "Numeric (Float)", "Numeric (Integer)"),
  "Possible Values" = c("Unique car brands (e.g., Porsche, Lamborghini, Ferrari)",
                        "Specific car models (e.g., 911, Huracan, 488 GTB)",
                        "Year of production, e.g., 2020, 2021",
                        "Engine size in liters, e.g., 3.0L, 4.0L",
                        "Horsepower, e.g., 500 HP, 1000 HP",
                        "Torque in lb-ft, e.g., 400 lb-ft, 800 lb-ft",
                        "Time in seconds, e.g., 2.5 sec, 3.5 sec",
                        "Price in USD, e.g., 50000, 1000000")
)

# Create the codebook table with knitr::kable() (this will format the table)
codebook_table <- kable(codebook_df, format = "html", 
                        col.names = c("Variable", "Description", "Data Type", "Possible Values"),
                        caption = "Codebook for Sports Car Prices Dataset")

# Save the codebook table as an HTML file
writeLines(codebook_table, "Sports_Car_Price_Codebook.html")

# Save the codebook table as a .txt file (tab-separated)
write.table(codebook_df, "Sports_Car_Price_Codebook.txt", sep = "\t", row.names = FALSE)

```

## Sources

This data was taken from the Kaggle data set called "Sports Car Prices dataset." The dataset can be found at this link: https://www.kaggle.com/datasets/rkiattisak/sports-car-prices-dataset. According to the data link, this dataset was large language models generated and not collected from actual data sources.

